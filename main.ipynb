{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67e01e8-9d70-4a16-b8b3-66b7e56ffa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wfdb in d:\\anaconda\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in d:\\anaconda\\lib\\site-packages (from wfdb) (3.11.18)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in d:\\anaconda\\lib\\site-packages (from wfdb) (2023.10.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in d:\\anaconda\\lib\\site-packages (from wfdb) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in d:\\anaconda\\lib\\site-packages (from wfdb) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.2.3 in d:\\anaconda\\lib\\site-packages (from wfdb) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.8.1 in d:\\anaconda\\lib\\site-packages (from wfdb) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.13.0 in d:\\anaconda\\lib\\site-packages (from wfdb) (1.15.2)\n",
      "Requirement already satisfied: soundfile>=0.10.0 in d:\\anaconda\\lib\\site-packages (from wfdb) (0.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anaconda\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (1.20.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas>=2.2.3->wfdb) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas>=2.2.3->wfdb) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.8.1->wfdb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.8.1->wfdb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.8.1->wfdb) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.8.1->wfdb) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\anaconda\\lib\\site-packages (from soundfile>=0.10.0->wfdb) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b136a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import medfilt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf1f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_by_averaging(signal, factor):\n",
    "    downsampled_signal = []\n",
    "    \n",
    "    for i in range(0, len(signal), factor): # moves by index of 8\n",
    "        chunk = signal[i:i+factor, :] \n",
    "        downsampled_signal.append(np.mean(chunk, axis=0))  # avg of each column\n",
    "        \n",
    "    return np.array(downsampled_signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df494514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Legion Pro/non-eeg-dataset-for-assessment-of-neurological-status-1.0.0/Subject1_AccTempEDA.hea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m ACCTEMPEDA_Path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_AccTempEDA\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      6\u001b[0m SPO2HR_Path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_SpO2HR\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m----> 8\u001b[0m ACCTEMPEDA_Record \u001b[38;5;241m=\u001b[39m wfdb\u001b[38;5;241m.\u001b[39mrdrecord(ACCTEMPEDA_Path) \u001b[38;5;66;03m#  rdrecord read .dat and .hea file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m SPO2HR_Record \u001b[38;5;241m=\u001b[39m wfdb\u001b[38;5;241m.\u001b[39mrdrecord(SPO2HR_Path) \n\u001b[0;32m     10\u001b[0m annotations \u001b[38;5;241m=\u001b[39m wfdb\u001b[38;5;241m.\u001b[39mrdann(ACCTEMPEDA_Path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124matr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# rdann reads .atr file\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\wfdb\\io\\record.py:2051\u001b[0m, in \u001b[0;36mrdrecord\u001b[1;34m(record_name, sampfrom, sampto, channels, physical, pn_dir, m2s, smooth_frames, ignore_skew, return_res, force_channels, channel_names, warn_empty)\u001b[0m\n\u001b[0;32m   2046\u001b[0m         dir_list \u001b[38;5;241m=\u001b[39m pn_dir\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2047\u001b[0m         pn_dir \u001b[38;5;241m=\u001b[39m posixpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   2048\u001b[0m             dir_list[\u001b[38;5;241m0\u001b[39m], download\u001b[38;5;241m.\u001b[39mget_version(dir_list[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m*\u001b[39mdir_list[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m   2049\u001b[0m         )\n\u001b[1;32m-> 2051\u001b[0m record \u001b[38;5;241m=\u001b[39m rdheader(record_name, pn_dir\u001b[38;5;241m=\u001b[39mpn_dir, rd_segments\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;66;03m# Set defaults for sampto and channels input variables\u001b[39;00m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampto \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;66;03m# If the header does not contain the signal length, figure it\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m     \u001b[38;5;66;03m# out from the first dat file. This is only possible for single\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m     \u001b[38;5;66;03m# segment records. If there are no signals, sig_len is 0.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\wfdb\\io\\record.py:1855\u001b[0m, in \u001b[0;36mrdheader\u001b[1;34m(record_name, pn_dir, rd_segments)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;66;03m# If it isn't a cloud path or a PhysioNet path, we treat as a local file\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1854\u001b[0m     dir_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(dir_name)\n\u001b[1;32m-> 1855\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m fsspec\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m   1856\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_name, file_name),\n\u001b[0;32m   1857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1858\u001b[0m         encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1859\u001b[0m         errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1860\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1861\u001b[0m         header_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1863\u001b[0m \u001b[38;5;66;03m# Separate comment and non-comment lines\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\fsspec\\core.py:100\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     98\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 100\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects \u001b[38;5;241m=\u001b[39m [f]\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\fsspec\\spec.py:1307\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1307\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1308\u001b[0m         path,\n\u001b[0;32m   1309\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1310\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1311\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1312\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1313\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1314\u001b[0m     )\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\fsspec\\implementations\\local.py:180\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\fsspec\\implementations\\local.py:302\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\fsspec\\implementations\\local.py:307\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    309\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Legion Pro/non-eeg-dataset-for-assessment-of-neurological-status-1.0.0/Subject1_AccTempEDA.hea'"
     ]
    }
   ],
   "source": [
    "data_list=[] #takes all the subjects \n",
    "directory=\"non-eeg-dataset-for-assessment-of-neurological-status-1.0.0\" # directory to all the files ATR, DAT, HEA\n",
    "csv_path = os.path.join(directory, 'subjectinfo.csv') \n",
    "for i in range(1,21):\n",
    "    ACCTEMPEDA_Path = os.path.join(directory, f'Subject{i}_AccTempEDA') \n",
    "    SPO2HR_Path = os.path.join(directory, f'Subject{i}_SpO2HR') \n",
    "\n",
    "    ACCTEMPEDA_Record = wfdb.rdrecord(ACCTEMPEDA_Path) #  rdrecord read .dat and .hea file\n",
    "    SPO2HR_Record = wfdb.rdrecord(SPO2HR_Path) \n",
    "    annotations = wfdb.rdann(ACCTEMPEDA_Path, 'atr') # rdann reads .atr file\n",
    "\n",
    "    # ACCTEMPEDA_DownSampled = downsample_by_averaging(ACCTEMPEDA_Record.p_signal, 8) # downsample ACCTEMP EDA as it is 8 HZ\n",
    "    # ACCTEMPEDA_DownSampled_Record = wfdb.Record(\n",
    "    #     record_name=f'Subject{i}_AccTempEDA_DownSampled', \n",
    "    #     p_signal=ACCTEMPEDA_DownSampled, \n",
    "    #     fs=1,\n",
    "    #     sig_name=ACCTEMPEDA_Record.sig_name,\n",
    "    #     units=ACCTEMPEDA_Record.units        \n",
    "    # )\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if int(row['subject']) == i:\n",
    "                data_list.append({\n",
    "                    'subject_id': i,\n",
    "                    'acc_temp_eda': ACCTEMPEDA_Record,\n",
    "                    'spo2_hr': SPO2HR_Record,\n",
    "                    'annotations': annotations,\n",
    "                    'age': int(row['age']),\n",
    "                    'gender': row['gender'],\n",
    "                    'height_cm': int(row['height/cm']),\n",
    "                    'weight_kg': int(row['weight/kg'])\n",
    "                })\n",
    "                #print(f\"Subject {i} Metadata:\")\n",
    "                #print(f\"  Age: {int(row['age'])}\")\n",
    "                #print(f\"  Gender: {row['gender']}\")\n",
    "                #print(f\"  Height (cm): {int(row['height/cm'])}\")\n",
    "                #print(f\"  Weight (kg): {int(row['weight/kg'])}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c32f8-8b08-440d-ad1e-21ddc69f7177",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = data_list[1]\n",
    "#print()\n",
    "print(sub['acc_temp_eda'].p_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = data_list[1]  \n",
    "wfdb.plot_wfdb( record=subject['acc_temp_eda'], annotation=subject['annotations'], time_units='minutes', title='Subject 1 - Downsampled AccTempEDA (1Hz)')\n",
    "wfdb.plot_wfdb( record=subject['spo2_hr'], annotation=subject['annotations'], time_units='minutes', title='Subject 1 - SpO2HR Signals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c0e45-e0c2-42e3-a2ac-9223ada72f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(sub['acc_temp_eda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ca48a-28e5-4e4e-8c0c-1d47bf30f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['acc_temp_eda'].p_signal = preprocess(sub['acc_temp_eda'].p_signal)\n",
    "wfdb.plot_wfdb( record=sub['acc_temp_eda'], annotation=subject['annotations'], time_units='seconds', title='Subject 1 - median filtered AccTempEDA (1Hz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716110bd-879f-40a3-a4e3-eb2187d1debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(sub['acc_temp_eda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3273101-2326-491d-acd2-add8b4917fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['acc_temp_eda'].p_signal = preprocess(sub['acc_temp_eda'])\n",
    "wfdb.plot_wfdb( record=sub['acc_temp_eda'], annotation=subject['annotations'], time_units='seconds', title='Subject 1 - gaussian filtered AccTempEDA (1Hz)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0c221-d3cc-4263-8a69-d33b0df9ed57",
   "metadata": {},
   "source": [
    "# Extracting and Combining Data by Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537a7a5-41a3-46ce-a6f2-79413b897c93",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43c655a-f58d-4e41-9fc7-9e1eea7aa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the ranges of each stage from their annotation\n",
    "def get_stages(annotation, size):\n",
    "    stages = []\n",
    "    for i, (samp, note) in enumerate(zip(annotation.sample, annotation.aux_note)):\n",
    "        stages.append({\"Label\" : note, \"start\": samp, \"end\": 0}) # dummy end\n",
    "    \n",
    "    for i in range((len(stages) - 1)):\n",
    "        stages[i][\"end\"] = stages[i+1][\"start\"] # each stages end is the next one's beginning\n",
    "    stages[-1][\"end\"] = size # set the end of last stage as the length of the data\n",
    "\n",
    "    return stages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4412628-2f72-4c72-89e4-cbf4b1e7d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find label of a data point at a given index based on the ranges of each stage\n",
    "def find_label(stages, index):\n",
    "    for stage in stages:\n",
    "        if index >= stage['start'] and index < stage['end']:\n",
    "            return stage[\"Label\"]\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc20640-9afa-4d12-bc1c-a99e043a3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the AccTempEDA data\n",
    "def extract_labeled_AccTempEDA(record, annotation):\n",
    "    stages = get_stages(annotation, record.p_signal.shape[0]) # get the range for each stage\n",
    "    data = record.p_signal # extract the data into a numpy array\n",
    "    labels = [] # array to store the labels corresponding to each data point in the record\n",
    "    for idx, entry in enumerate(data): # loop over all the data\n",
    "        labels.append((find_label(stages, idx))) # find the label based on the points index\n",
    "    labeled_data = np.column_stack((data, labels)) # append the labels column to the data \n",
    "    return labeled_data # return the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ec0707-8dc6-487b-a76a-f8b7b58ff552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample by averaging BUT make sure you stay within a given class\n",
    "def downsample_by_averaging(data, factor):\n",
    "    downsampled_data = []\n",
    "    i = 0\n",
    "    while i + factor <= len(data): # cannot use for (range) as we may need to update i\n",
    "        class_label = data[i][-1] # get the expected class lavel\n",
    "        if class_label == data[i + factor - 1][-1]: # if all the data belongs to one class\n",
    "            chunk = data[i: i + factor, :-1].astype(float) # extract a chunk (ignore last column [label] to average and cast to float to counter the upcasting when we added the label)\n",
    "            avg_chunk = np.mean(chunk, axis=0)\n",
    "            downsampled_data.append(np.append(avg_chunk, class_label))  # avg of each column while adding back the label\n",
    "            i += factor\n",
    "        else: # we have crossed into a new class, need to reset i to beginning of the new class\n",
    "            for j in range(i + 1, i + factor):\n",
    "                if data[j-1][-1] != data[j][-1]: # if data at index j has a different class from the one before it, update i and break\n",
    "                    i = j\n",
    "                    break\n",
    "                    \n",
    "    return np.array(downsampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a01bc0-5b4c-4c14-bfaf-1c059177d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine AccTempEDA and Spo2HR data while truncating them to be of the same length\n",
    "# CRUCIAL but naive assumption! time stamps match after downsampling (technically incorrect)\n",
    "def combine(AccTempEDA, Spo2HR):\n",
    "    min_length = min(len(AccTempEDA), len(Spo2HR)) # finds the minimum length to align the points properly\n",
    "\n",
    "    # concatenate the truncated columns from AccTempEDA with those from Spo2HR and add the labels at the end\n",
    "    combined_data = np.column_stack((AccTempEDA[:min_length, :-1], Spo2HR[:min_length], AccTempEDA[:min_length, -1:]))\n",
    "\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba5071c-119b-47e3-8b20-c0d5764807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(combined_data):\n",
    "    balanced_data = []\n",
    "    relaxed = False\n",
    "    for data in combined_data:\n",
    "        if data[-1] != \"Relax\": # we've passed the first relax\n",
    "            relaxed = True\n",
    "            balanced_data.append(data)\n",
    "        elif not relaxed:\n",
    "            balanced_data.append(data)\n",
    "    return np.array(balanced_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73be24-0eed-4023-9901-125fd8e1ac42",
   "metadata": {},
   "source": [
    "# Extracting Raw and Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0039b668-b876-43da-ab90-04455083d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val is 1 incase labels are included 0 otherwise \n",
    "def preprocess(wavevec):\n",
    "    final_vec = [] # stores the filtered signals\n",
    "    for i in range(0,len(wavevec[0])): # goes through each point in the signal\n",
    "        one_col = [wavevec[s][i].astype(float) for s in range(0,len(wavevec))] # filters column by column\n",
    "        one_col = medfilt(one_col, kernel_size=9) # filtering each column\n",
    "        if i == 0:\n",
    "            final_vec = one_col\n",
    "        else:\n",
    "            final_vec = np.column_stack((final_vec, one_col))\n",
    "    return final_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "096c06d3-a103-4f51-860b-018043175c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the full data for each subject\n",
    "def get_subject_data_preprocessed(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation):\n",
    "    AccTempEDA_record.p_signal = preprocess(AccTempEDA_record.p_signal)\n",
    "    AccTempEDA = extract_labeled_AccTempEDA(AccTempEDA_record, AccTempEDA_annotation) # extract and label AccTempEDA record\n",
    "    #print(AccTempEDA[3000])\n",
    "    AccTempEDA = downsample_by_averaging(AccTempEDA, 8) # downsample to match the other Spo2HR's rate\n",
    "    Spo2HR = preprocess(Spo2HR_record.p_signal) # extract Spo2HR record\n",
    "    #print(Spo2HR.shape)\n",
    "    combined_data = combine(AccTempEDA, Spo2HR) # combine both records\n",
    "    #print(len(combined_data))\n",
    "    return combined_data # return the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "296cfbde-f324-4d4b-b04b-6510daf0d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_data_raw(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation):\n",
    "    AccTempEDA = extract_labeled_AccTempEDA(AccTempEDA_record, AccTempEDA_annotation) # extract and label AccTempEDA record\n",
    "    #print(AccTempEDA[3000])\n",
    "    #print(AccTempEDA)\n",
    "    AccTempEDA = downsample_by_averaging(AccTempEDA, 8) # downsample to match the other Spo2HR's rate\n",
    "    Spo2HR = Spo2HR_record.p_signal # extract Spo2HR record\n",
    "    #print(Spo2HR.shape)\n",
    "    combined_data = combine(AccTempEDA, Spo2HR) # combine both records\n",
    "    #print(len(combined_data))\n",
    "    return combined_data # return the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666d543f-3311-4171-9aab-11c257a8a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_data_raw_balanced(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation):\n",
    "    AccTempEDA = extract_labeled_AccTempEDA(AccTempEDA_record, AccTempEDA_annotation) # extract and label AccTempEDA record\n",
    "    #print(AccTempEDA[3000])\n",
    "    #print(AccTempEDA)\n",
    "    AccTempEDA = downsample_by_averaging(AccTempEDA, 8) # downsample to match the other Spo2HR's rate\n",
    "    Spo2HR = Spo2HR_record.p_signal # extract Spo2HR record\n",
    "    #print(Spo2HR.shape)\n",
    "    combined_data = combine(AccTempEDA, Spo2HR) # combine both records\n",
    "    balanced_data = balance_classes(combined_data)\n",
    "    return balanced_data # return the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "330b54fa-98c3-4998-bac5-9f697d005fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the full data for each subject\n",
    "def get_subject_data_preprocessed_balanced(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation):\n",
    "    AccTempEDA_record.p_signal = preprocess(AccTempEDA_record.p_signal)\n",
    "    AccTempEDA = extract_labeled_AccTempEDA(AccTempEDA_record, AccTempEDA_annotation) # extract and label AccTempEDA record\n",
    "    #print(AccTempEDA[3000])\n",
    "    AccTempEDA = downsample_by_averaging(AccTempEDA, 8) # downsample to match the other Spo2HR's rate\n",
    "    Spo2HR = preprocess(Spo2HR_record.p_signal) # extract Spo2HR record\n",
    "    #print(Spo2HR.shape)\n",
    "    combined_data = combine(AccTempEDA, Spo2HR) # combine both records\n",
    "    balanced_data = balance_classes(combined_data)\n",
    "    return balanced_data # return the resulting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a1c61-e23a-4fab-b308-ed091c60f757",
   "metadata": {},
   "source": [
    "# Function to read and get raw data for all subjects and store them in csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1f124-548c-478e-a10f-f2e82f152c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "read_directory=\"non-eeg-dataset-for-assessment-of-neurological-status-1.0.0\" # directory to all the files ATR, DAT, HEA\n",
    "type = \"Preprocessed\"\n",
    "write_directory= f\"Subject Data {type}\"\n",
    "all_sub_info = []\n",
    "for i in range(1,21):\n",
    "    csv_path = os.path.join(write_directory, f'subject_{i}_data_{type}.csv')\n",
    "    ACCTEMPEDA_Path = os.path.join(read_directory, f'Subject{i}_AccTempEDA') \n",
    "    SPO2HR_Path = os.path.join(read_directory, f'Subject{i}_SpO2HR') \n",
    "\n",
    "    ACCTEMPEDA_Record = wfdb.rdrecord(ACCTEMPEDA_Path) #  rdrecord read .dat and .hea file\n",
    "    SPO2HR_Record = wfdb.rdrecord(SPO2HR_Path) \n",
    "    annotations = wfdb.rdann(ACCTEMPEDA_Path, 'atr') # rdann reads .atr file\n",
    "    subject_data = get_subject_data_preprocessed(ACCTEMPEDA_Record, SPO2HR_Record, annotations)\n",
    "    all_sub_info.append(subject_data)\n",
    "    # #print(f'subject{i}: {len(subject_data)}') \n",
    "    # with open(csv_path, mode='w', newline='') as csvfile:\n",
    "    #     writer = csv.writer(csvfile)\n",
    "    #     # Write headers\n",
    "    #     writer.writerow(['ax', 'ay', 'az', 'temp', 'EDA', 'SpO2', 'HR', 'label'])\n",
    "    #     # Write each row of subject data\n",
    "    #     for row in subject_data:\n",
    "    #         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f91fcb-49f6-4018-a334-83df44c77e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sub_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2278251-b505-429e-8e58-7c327c8cc6b3",
   "metadata": {},
   "source": [
    "# Function to get the data in the desired window sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628bdb47-4077-4cf8-8ef3-bac042affb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowSel(data, factor):\n",
    "    # stores the windowed sample\n",
    "    windowed = []\n",
    "    labels = []\n",
    "    # stores the last i in case the window exceeds the size of the sample\n",
    "    for i in range(0, len(data), factor):\n",
    "        # checks if i exceeds the sample size\n",
    "        if i + factor < len(data):\n",
    "            class_label = data[i][-1] # get the expected class lavel\n",
    "            if class_label == data[i + factor - 1][-1]: # if all the data belongs to one class\n",
    "            # adds the windowed sample \n",
    "                windowed.append(data[i: i + factor, :-1])\n",
    "                labels.append(class_label)\n",
    "            else:\n",
    "                continue     \n",
    "    return windowed, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a684e-8514-426e-8bfd-1ffc3176882e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "windowed_samps = []\n",
    "val = 5\n",
    "for i in range(0,20):\n",
    "   windowSel(all_sub_info[1],val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16556ce-7d9c-4e03-bb12-9a81aa50e7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde2bdb-00cc-4108-9362-ee2b515ad049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de803d9f-7fb7-46cf-9596-41612102713e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4e9d9-93bd-4a33-b780-d37e7ffdb6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c3137de-d3fc-40c4-80c2-59cd1d85bda1",
   "metadata": {},
   "source": [
    "# Compute Z score [Normalization] (importnat for methods like KNN, logistic regression, SVM and neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591e6ea-df49-446e-8504-2cf5868c277f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8fa12-3bc5-409c-bd1c-9b3fb48e61a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec08ccde-2963-465d-84d1-810912ea7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data includes the features without the labels column\n",
    "def normalize_data(train_data, test_data):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data)\n",
    "    X_test_scaled = scaler.transform(test_data)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9a8db-1fd6-4e9e-b964-ddd04b66c399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f039ebf0-4486-4980-a59e-73c04299c826",
   "metadata": {},
   "source": [
    "# Getting Data Raw, Preprocessed (Balanced and Unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59638ce3-e67a-47d5-b072-906af7a0f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "872c4574-d9c0-48b1-885f-beb4e06b60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = rf\"D:\\University\\8.Spring 2025\\Machine\\Project\\non-eeg-dataset-for-assessment-of-neurological-status-1.0.0\\non-eeg-dataset-for-assessment-of-neurological-status-1.0.0\"\n",
    "all_preprocessed_data_balanced=[]\n",
    "all_preprocessed_data=[]\n",
    "all_raw_data_balanced=[]\n",
    "all_raw_data=[]\n",
    "for i in range(1, 21):\n",
    "    AccTempEDA_record = wfdb.rdrecord(fr\"{data_dir}\\Subject{i}_AccTempEDA\")\n",
    "    Spo2HR_record = wfdb.rdrecord(fr\"{data_dir}\\Subject{i}_SpO2HR\")\n",
    "    AccTempEDA_annotation = wfdb.rdann(fr\"{data_dir}\\Subject{i}_AccTempEDA\", 'atr')\n",
    "    \n",
    "    data = get_subject_data_preprocessed(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation)\n",
    "    raw_data = get_subject_data_raw(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation)\n",
    "    data_balanced = get_subject_data_preprocessed_balanced(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation)\n",
    "    raw_data_balanced = get_subject_data_raw_balanced(AccTempEDA_record, Spo2HR_record, AccTempEDA_annotation)\n",
    "    \n",
    "    all_preprocessed_data.append(data)\n",
    "    all_raw_data.append(raw_data)\n",
    "    all_preprocessed_data_balanced.append(data_balanced)\n",
    "    all_raw_data_balanced.append(raw_data_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8896e6-9349-43a3-bc61-973446c6efdb",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0aabf8b2-3de9-4b43-8358-1121f43b32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squared_all_subs(data, norm = False):\n",
    "    precision_list_squares = []\n",
    "    recall_list_squares = []\n",
    "    f1_list_squares = []\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        X, y = data[i - 1][:, :-1].astype(float), data[i - 1][:, -1]\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        # Prepare lists to store metrics for each fold\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            if(norm):\n",
    "                X_train, X_test = normalize_data(X_train, X_test)\n",
    "                \n",
    "            clf = RidgeClassifier()\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "        \n",
    "        # Final combined classification report\n",
    "        # print(f\"Subject {i}\")\n",
    "        # print(classification_report(all_true, all_preds, zero_division=0))\n",
    "        # print(\"\\n\")\n",
    "    \n",
    "        report_dict = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "        macro_avg = report_dict['macro avg']\n",
    "    \n",
    "        avg_precision = macro_avg['precision']\n",
    "        avg_recall = macro_avg['recall']\n",
    "        avg_f1_score = macro_avg['f1-score']\n",
    "    \n",
    "        precision_list_squares.append(avg_precision)\n",
    "        recall_list_squares.append(avg_recall)\n",
    "        f1_list_squares.append(avg_f1_score)\n",
    "        \n",
    "    overall_avg_precision_squares = sum(precision_list_squares) / len(precision_list_squares)\n",
    "    overall_avg_recall_squares = sum(recall_list_squares) / len(recall_list_squares)\n",
    "    overall_avg_f1_squares = sum(f1_list_squares) / len(f1_list_squares)\n",
    "    \n",
    "    print(\"Overall averages for 20 subjects with Least Sqaures classifier:\")\n",
    "    print(f\"Average Precision: {overall_avg_precision_squares:.3f}\")\n",
    "    print(f\"Average Recall: {overall_avg_recall_squares:.3f}\")\n",
    "    print(f\"Average F1-Score: {overall_avg_f1_squares:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f5028-72fa-48b3-8d1f-9b4c2237b4c0",
   "metadata": {},
   "source": [
    "## Least Squares on Raw Data (Unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43120efc-07af-4568-98cf-7cb85b0b4d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Least Sqaures classifier:\n",
      "Average Precision: 0.783\n",
      "Average Recall: 0.766\n",
      "Average F1-Score: 0.765\n"
     ]
    }
   ],
   "source": [
    "least_squared_all_subs(all_raw_data, norm = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877fad8-ca8d-4d41-b4b4-7f801d36dc1c",
   "metadata": {},
   "source": [
    "## Least Squares on Raw Data (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "914d31d4-8724-4799-829a-98d15d4f3e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Least Sqaures classifier:\n",
      "Average Precision: 0.912\n",
      "Average Recall: 0.908\n",
      "Average F1-Score: 0.906\n"
     ]
    }
   ],
   "source": [
    "least_squared_all_subs(all_raw_data_balanced, norm = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177086db-5667-4b1c-841a-d6bdc502a75c",
   "metadata": {},
   "source": [
    "## Least Squares on Preprocessed (Unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc5d27f-77e3-45b9-af7b-97ca9c3ac85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Least Sqaures classifier:\n",
      "Average Precision: 0.836\n",
      "Average Recall: 0.814\n",
      "Average F1-Score: 0.793\n"
     ]
    }
   ],
   "source": [
    "least_squared_all_subs(all_preprocessed_data, norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e4182-c68e-453f-846a-7a3931c3b1c1",
   "metadata": {},
   "source": [
    "## Least Squares on Preprocessed (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40a96444-e34f-4364-93a9-c52e48093227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Least Sqaures classifier:\n",
      "Average Precision: 0.931\n",
      "Average Recall: 0.912\n",
      "Average F1-Score: 0.907\n"
     ]
    }
   ],
   "source": [
    "least_squared_all_subs(all_preprocessed_data_balanced, norm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7c886-a8ba-4967-b5c3-c9a6039c68f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce5d307-01cd-4f66-a643-69da4f5d58c9",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6097db7-1136-48e7-9c78-c36454e6bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_all_subs(data, norm = False):\n",
    "    precision_list_squares = []\n",
    "    recall_list_squares = []\n",
    "    f1_list_squares = []\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        X, y = data[i - 1][:, :-1].astype(float), data[i - 1][:, -1]\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        # Prepare lists to store metrics for each fold\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            if(norm):\n",
    "                X_train, X_test = normalize_data(X_train, X_test)\n",
    "                \n",
    "            clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "        \n",
    "        # Final combined classification report\n",
    "        # print(f\"Subject {i}\")\n",
    "        # print(classification_report(all_true, all_preds, zero_division=0))\n",
    "        # print(\"\\n\")\n",
    "    \n",
    "        report_dict = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "        macro_avg = report_dict['macro avg']\n",
    "    \n",
    "        avg_precision = macro_avg['precision']\n",
    "        avg_recall = macro_avg['recall']\n",
    "        avg_f1_score = macro_avg['f1-score']\n",
    "    \n",
    "        precision_list_squares.append(avg_precision)\n",
    "        recall_list_squares.append(avg_recall)\n",
    "        f1_list_squares.append(avg_f1_score)\n",
    "        \n",
    "    overall_avg_precision_squares = sum(precision_list_squares) / len(precision_list_squares)\n",
    "    overall_avg_recall_squares = sum(recall_list_squares) / len(recall_list_squares)\n",
    "    overall_avg_f1_squares = sum(f1_list_squares) / len(f1_list_squares)\n",
    "    \n",
    "    print(\"Overall averages for 20 subjects with Logisitic Regression:\")\n",
    "    print(f\"Average Precision: {overall_avg_precision_squares:.3f}\")\n",
    "    print(f\"Average Recall: {overall_avg_recall_squares:.3f}\")\n",
    "    print(f\"Average F1-Score: {overall_avg_f1_squares:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692634b-6c6e-4d71-afca-710164477743",
   "metadata": {},
   "source": [
    "## doesn't work without normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426f80f-69d6-4bc6-9cd1-8e3e2be5e250",
   "metadata": {},
   "source": [
    "## Logistic Regression on Preprocessed (Unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b952de6d-4c99-44dd-bd0e-a9154273f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Logisitic Regression:\n",
      "Average Precision: 0.809\n",
      "Average Recall: 0.827\n",
      "Average F1-Score: 0.813\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_all_subs(all_preprocessed_data, norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664dc0d-99c7-4b60-935b-8aececdb5860",
   "metadata": {},
   "source": [
    "## Logistic Regression on Preprocessed (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3859a98f-ca2b-4401-a93d-7e4f14611e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Logisitic Regression:\n",
      "Average Precision: 0.941\n",
      "Average Recall: 0.940\n",
      "Average F1-Score: 0.939\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_all_subs(all_preprocessed_data_balanced, norm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751c00e-0c4a-4ec0-bac6-4e7e5ab72878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f407a2d5-f8f4-422e-b89c-1f67210f58c1",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "764d05a5-b27c-4933-8938-0324830382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_all_subs(data, k, norm = False):\n",
    "    precision_list_squares = []\n",
    "    recall_list_squares = []\n",
    "    f1_list_squares = []\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        X, y = data[i - 1][:, :-1].astype(float), data[i - 1][:, -1]\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        # Prepare lists to store metrics for each fold\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            if(norm):\n",
    "                X_train, X_test = normalize_data(X_train, X_test)\n",
    "                \n",
    "            clf = SVC(kernel= k, decision_function_shape='ovr')\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "        \n",
    "        # Final combined classification report\n",
    "        # print(f\"Subject {i}\")\n",
    "        # print(classification_report(all_true, all_preds, zero_division=0))\n",
    "        # print(\"\\n\")\n",
    "    \n",
    "        report_dict = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "        macro_avg = report_dict['macro avg']\n",
    "    \n",
    "        avg_precision = macro_avg['precision']\n",
    "        avg_recall = macro_avg['recall']\n",
    "        avg_f1_score = macro_avg['f1-score']\n",
    "    \n",
    "        precision_list_squares.append(avg_precision)\n",
    "        recall_list_squares.append(avg_recall)\n",
    "        f1_list_squares.append(avg_f1_score)\n",
    "        \n",
    "    overall_avg_precision_squares = sum(precision_list_squares) / len(precision_list_squares)\n",
    "    overall_avg_recall_squares = sum(recall_list_squares) / len(recall_list_squares)\n",
    "    overall_avg_f1_squares = sum(f1_list_squares) / len(f1_list_squares)\n",
    "    \n",
    "    print(f\"Overall averages for 20 subjects with SVM {k}:\")\n",
    "    print(f\"Average Precision: {overall_avg_precision_squares:.3f}\")\n",
    "    print(f\"Average Recall: {overall_avg_recall_squares:.3f}\")\n",
    "    print(f\"Average F1-Score: {overall_avg_f1_squares:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0d40c-ccda-4a5a-8d8b-57031ee1c745",
   "metadata": {},
   "source": [
    "## Unnormalized also doesn't run (takes too long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc9123-44f4-47b0-b23e-c52f4e77a4dc",
   "metadata": {},
   "source": [
    "### Linear SVM on Preprocessed (UnBalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87048227-1b88-4c88-9c94-91f6f04aa93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with SVM linear:\n",
      "Average Precision: 0.831\n",
      "Average Recall: 0.856\n",
      "Average F1-Score: 0.837\n"
     ]
    }
   ],
   "source": [
    "svm_all_subs(all_preprocessed_data, 'linear', norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b238fb3-f3e1-4817-ad9c-4d619fb805a7",
   "metadata": {},
   "source": [
    "### Linear SVM on Preprocessed (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "467ff2c7-cfb4-4a1b-b296-a642dc0b3071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with SVM linear:\n",
      "Average Precision: 0.948\n",
      "Average Recall: 0.948\n",
      "Average F1-Score: 0.946\n"
     ]
    }
   ],
   "source": [
    "svm_all_subs(all_preprocessed_data_balanced, 'linear', norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53123ee3-3fdd-4e57-a242-c879beaa70a8",
   "metadata": {},
   "source": [
    "### Non-linear SVM on Preprocessed (UnBalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce222779-bf76-445b-bf5b-a13ac1d0b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with SVM rbf:\n",
      "Average Precision: 0.866\n",
      "Average Recall: 0.882\n",
      "Average F1-Score: 0.869\n"
     ]
    }
   ],
   "source": [
    "svm_all_subs(all_preprocessed_data, 'rbf', norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7a860-eed9-4c51-bd73-6dd5891d6208",
   "metadata": {},
   "source": [
    "### Non-linear SVM on Preprocessed (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13eac6eb-9a07-4193-b7eb-0e380ffd43a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with SVM rbf:\n",
      "Average Precision: 0.949\n",
      "Average Recall: 0.950\n",
      "Average F1-Score: 0.948\n"
     ]
    }
   ],
   "source": [
    "svm_all_subs(all_preprocessed_data_balanced, 'rbf', norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe268c82-0f79-4a04-95c0-26b110bf4cb1",
   "metadata": {},
   "source": [
    "#### Observation (little difference between linear and non-linear) since high dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebaf74-f69b-4b1a-8787-0acb470fd6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3b88a13-4854-41a4-bef0-ce7051a7c3f4",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9be5fa12-dfb0-4004-9374-c1ed173e5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_all_subs(data, norm = False):\n",
    "    precision_list_squares = []\n",
    "    recall_list_squares = []\n",
    "    f1_list_squares = []\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        X, y = data[i - 1][:, :-1].astype(float), data[i - 1][:, -1]\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        # Prepare lists to store metrics for each fold\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            if(norm):\n",
    "                X_train, X_test = normalize_data(X_train, X_test)\n",
    "                \n",
    "            clf = GradientBoostingClassifier()\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "        \n",
    "        # Final combined classification report\n",
    "        # print(f\"Subject {i}\")\n",
    "        # print(classification_report(all_true, all_preds, zero_division=0))\n",
    "        # print(\"\\n\")\n",
    "    \n",
    "        report_dict = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "        macro_avg = report_dict['macro avg']\n",
    "    \n",
    "        avg_precision = macro_avg['precision']\n",
    "        avg_recall = macro_avg['recall']\n",
    "        avg_f1_score = macro_avg['f1-score']\n",
    "    \n",
    "        precision_list_squares.append(avg_precision)\n",
    "        recall_list_squares.append(avg_recall)\n",
    "        f1_list_squares.append(avg_f1_score)\n",
    "        \n",
    "    overall_avg_precision_squares = sum(precision_list_squares) / len(precision_list_squares)\n",
    "    overall_avg_recall_squares = sum(recall_list_squares) / len(recall_list_squares)\n",
    "    overall_avg_f1_squares = sum(f1_list_squares) / len(f1_list_squares)\n",
    "    \n",
    "    print(\"Overall averages for 20 subjects with Gradient Boosting:\")\n",
    "    print(f\"Average Precision: {overall_avg_precision_squares:.3f}\")\n",
    "    print(f\"Average Recall: {overall_avg_recall_squares:.3f}\")\n",
    "    print(f\"Average F1-Score: {overall_avg_f1_squares:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e85be9b4-5710-4a17-bddd-ff5add4d3691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Gradient Boosting:\n",
      "Average Precision: 0.866\n",
      "Average Recall: 0.880\n",
      "Average F1-Score: 0.869\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting_all_subs(all_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80618992-4135-4b21-9a58-4496dc9c1ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Gradient Boosting:\n",
      "Average Precision: 0.931\n",
      "Average Recall: 0.922\n",
      "Average F1-Score: 0.924\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting_all_subs(all_raw_data_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c860018-29bd-4521-98bf-158b15080314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Gradient Boosting:\n",
      "Average Precision: 0.869\n",
      "Average Recall: 0.880\n",
      "Average F1-Score: 0.871\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting_all_subs(all_preprocessed_data, norm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81d0f3a5-678b-4b27-a249-4af88ddca5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall averages for 20 subjects with Gradient Boosting:\n",
      "Average Precision: 0.931\n",
      "Average Recall: 0.922\n",
      "Average F1-Score: 0.923\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting_all_subs(all_preprocessed_data_balanced, norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7f0db-c609-4288-ab7b-c345b00cc674",
   "metadata": {},
   "source": [
    "### Note: no difference with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9b70a-248a-4e04-8501-29a36a9ca473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96300f90-fc84-49de-b7cc-3684606cab75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
